{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as '_psutil_linux' could not be imported from 'most likely due to a circular import'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# datasets.py\n",
    "# This script contains the dataset class and data loading functionality.\n",
    "\n",
    "import h5py  # Library for handling HDF5 files\n",
    "import pandas as pd  # DataFrame handling\n",
    "import numpy as np  # Numerical operations\n",
    "import torch  # PyTorch for tensor operations and neural networks\n",
    "from torch.utils.data import Dataset, DataLoader  # Dataset and DataLoader classes from PyTorch\n",
    "from torchvision import transforms  # Transformations for image data\n",
    "from PIL import Image  # Image handling from the Python Imaging Library (PIL)\n",
    "import io  # Input/output library for byte handling\n",
    "\n",
    "# Define a custom dataset class for handling skin cancer data\n",
    "class SkinCancerDataset(Dataset):\n",
    "    def __init__(self, metadata_file, hdf5_file, transform=None):\n",
    "        self.metadata = pd.read_csv(metadata_file)  # Load metadata CSV file\n",
    "        self.hdf5_file = h5py.File(hdf5_file, 'r')  # Open the HDF5 file containing images\n",
    "        self.transform = transform  # Store the transformation to be applied on images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)  # Return the number of samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.metadata.iloc[idx]['isic_id']  # Get image ID from metadata\n",
    "        label = self.metadata.iloc[idx]['target']  # Get label (target) from metadata\n",
    "\n",
    "        # Check if the image ID is in the HDF5 file\n",
    "        if image_id in self.hdf5_file:\n",
    "            image = self.hdf5_file[image_id][()]  # Load the image from HDF5 file\n",
    "\n",
    "            if isinstance(image, np.bytes_):\n",
    "                # If image data is in bytes format, decode it\n",
    "                image = np.frombuffer(image, dtype=np.uint8)\n",
    "                image = Image.open(io.BytesIO(image))  # Open the image from bytes\n",
    "            elif isinstance(image, np.ndarray):\n",
    "                # If image data is a numpy array, convert it to PIL Image\n",
    "                image = Image.fromarray(image)\n",
    "            else:\n",
    "                raise TypeError(f\"Unexpected data format for image ID {image_id}. Expected numpy array or bytes, got {type(image)}.\")\n",
    "\n",
    "            image = image.convert(\"RGB\")  # Ensure the image is in RGB format\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)  # Apply transformations if specified\n",
    "\n",
    "            return image, torch.tensor(label, dtype=torch.float32)  # Return the transformed image and label as a tensor\n",
    "        else:\n",
    "            raise KeyError(f\"Image ID {image_id} not found in HDF5 file.\")  # Raise an error if the image is not found\n",
    "\n",
    "    def __del__(self):\n",
    "        self.hdf5_file.close()  # Ensure the HDF5 file is closed when the dataset object is deleted\n",
    "\n",
    "# Function to get a DataLoader for the dataset\n",
    "def get_dataloader(metadata_file, hdf5_file, batch_size, mean, std, augment=False):\n",
    "    if augment:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),  # Randomly crop and resize images\n",
    "            transforms.RandomHorizontalFlip(),  # Randomly flip images horizontally\n",
    "            transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "            transforms.Normalize(mean=mean, std=std),  # Normalize images with mean and std deviation\n",
    "        ])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(256),  # Resize images\n",
    "            transforms.CenterCrop(224),  # Center crop images\n",
    "            transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "            transforms.Normalize(mean=mean, std=std),  # Normalize images with mean and std deviation\n",
    "        ])\n",
    "    \n",
    "    dataset = SkinCancerDataset(metadata_file, hdf5_file, transform)  # Create the dataset\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)  # Create the DataLoader\n",
    "    \n",
    "    return dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as '_psutil_linux' could not be imported from 'most likely due to a circular import'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Model.py\n",
    "# This script defines the model architecture.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn  # Import neural network module from PyTorch\n",
    "import torchvision.models as models  # Import pre-trained models from torchvision\n",
    "\n",
    "# Define a model class for skin cancer detection\n",
    "class SkinCancerModel(nn.Module):\n",
    "    def __init__(self, model_name='efficientnet_v2_m', pretrained=True):\n",
    "        super(SkinCancerModel, self).__init__()\n",
    "        \n",
    "        # Load a pre-trained EfficientNetV2 model\n",
    "        if model_name == 'efficientnet_v2_m':\n",
    "            self.backbone = models.efficientnet_v2_m(weights=models.EfficientNet_V2_M_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "        elif model_name == 'efficientnet_v2_s':\n",
    "            self.backbone = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "        else:\n",
    "            raise ValueError(f\"Model {model_name} is not supported\")  # Raise an error if the model name is not supported\n",
    "        \n",
    "        # Modify the classifier to fit our binary classification task\n",
    "        num_features = self.backbone.classifier[1].in_features  # Get the number of input features to the classifier\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Linear(num_features, 1)  # Replace the classifier with a single linear layer for binary classification\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)  # Define the forward pass of the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as '_psutil_linux' could not be imported from 'most likely due to a circular import'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# utils.py\n",
    "import torch\n",
    "import torch.optim as optim  # Import optimization algorithms\n",
    "import torch.nn as nn  # Import neural network module\n",
    "from tqdm import tqdm  # Progress bar\n",
    "from torch.cuda.amp import GradScaler, autocast  # Mixed precision training tools\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, train_loader, test_loader, num_epochs, learning_rate, save_path, device, accumulation_steps=4):\n",
    "    model.to(device)  # Move the model to the specified device (GPU or CPU)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()  # Define the loss function (binary cross-entropy with logits)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Define the optimizer (Adam)\n",
    "    scaler = GradScaler()  # Initialize the GradScaler for mixed precision\n",
    "    \n",
    "    best_val_loss = float('inf')  # Initialize the best validation loss with infinity\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Iterate over the training data\n",
    "        for step, (images, targets) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)):\n",
    "            images, targets = images.to(device), targets.to(device)  # Move images and targets to the device\n",
    "            \n",
    "            with autocast():  # Enable mixed precision\n",
    "                outputs = model(images)  # Forward pass\n",
    "                loss = criterion(outputs, targets.unsqueeze(1))  # Compute the loss\n",
    "                loss = loss / accumulation_steps  # Normalize the loss\n",
    "            \n",
    "            scaler.scale(loss).backward()  # Scale the gradients and perform backward pass\n",
    "            \n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)  # Update the weights\n",
    "                scaler.update()  # Update the scaler\n",
    "                optimizer.zero_grad()  # Zero the gradients\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)  # Accumulate the running loss\n",
    "        \n",
    "        torch.cuda.empty_cache()  # Clean up GPU memory\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)  # Compute the epoch loss\n",
    "        \n",
    "        val_loss = validate_model(model, test_loader, criterion, device)  # Validate the model\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.4f} - Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Save the model if the validation loss improves\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_model(model, save_path)\n",
    "\n",
    "# Function to validate the model\n",
    "def validate_model(model, test_loader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for images, targets in test_loader:\n",
    "            images, targets = images.to(device), targets.to(device)  # Move images and targets to the device\n",
    "            \n",
    "            with autocast():  # Enable mixed precision\n",
    "                outputs = model(images)  # Forward pass\n",
    "                loss = criterion(outputs, targets.unsqueeze(1))  # Compute the loss\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)  # Accumulate the running loss\n",
    "    \n",
    "    val_loss = running_loss / len(test_loader.dataset)  # Compute the validation loss\n",
    "    return val_loss\n",
    "\n",
    "# Function to save the model\n",
    "def save_model(model, save_path):\n",
    "    torch.save(model.state_dict(), save_path)  # Save the model's state dictionary to the specified path\n",
    "    print(f\"Model saved to {save_path}\")  # Print a message indicating that the model was saved\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as '_psutil_linux' could not be imported from 'most likely due to a circular import'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# inference.py\n",
    "\n",
    "import pandas as pd  # Import pandas for data handling\n",
    "import torch  # Import PyTorch\n",
    "from torchvision import transforms, models  # Import transformations and models\n",
    "from torch.utils.data import DataLoader  # Import DataLoader class\n",
    "from datasets import SkinCancerDataset  # Import the custom dataset class\n",
    "from model import SkinCancerModel  # Import the model class\n",
    "\n",
    "# Define the test function\n",
    "def test(model_path, test_metadata_file, test_hdf5_file, batch_size, mean, std, device):\n",
    "    # Initialize the transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),  # Resize the images\n",
    "        transforms.CenterCrop(224),  # Center crop the images\n",
    "        transforms.ToTensor(),  # Convert the images to tensors\n",
    "        transforms.Normalize(mean=mean, std=std)  # Normalize the images\n",
    "    ])\n",
    "    \n",
    "    # Load the test dataset\n",
    "    test_dataset = SkinCancerDataset(test_metadata_file, test_hdf5_file, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Initialize the model and load the trained weights\n",
    "    model = SkinCancerModel()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    \n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for images, _ in test_loader:\n",
    "            images = images.to(device)  # Move images to the device\n",
    "            outputs = model(images)  # Get the model predictions\n",
    "            predictions.append(outputs.cpu().numpy())  # Store the predictions\n",
    "\n",
    "    predictions = np.concatenate(predictions)  # Concatenate predictions\n",
    "    return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as '_psutil_linux' could not be imported from 'most likely due to a circular import'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "import torch\n",
    "from torchvision import models\n",
    "from datasets import get_dataloader\n",
    "from utils import train_model\n",
    "\n",
    "# Set device to 'cuda' (GPU) if available, otherwise fall back to 'cpu'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define the mean and standard deviation for image normalization\n",
    "# These values are commonly used for pre-trained models on ImageNet\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Set training parameters\n",
    "model_name = 'efficientnet_v2_m'  # Name of the model architecture to use\n",
    "num_epochs = 1  # Number of epochs to train the model\n",
    "batch_size = 2  # Number of samples processed per batch\n",
    "save_path = 'models/model.pth'  # File path where the trained model will be saved\n",
    "\n",
    "# Paths to the metadata and HDF5 files for training and testing datasets\n",
    "train_metadata_file = 'data/train-metadata.csv'  # Metadata file for training data\n",
    "train_hdf5_file = 'data/train-image.hdf5'  # HDF5 file containing training images\n",
    "test_metadata_file = 'data/test-metadata.csv'  # Metadata file for testing data\n",
    "test_hdf5_file = 'data/test-image.hdf5'  # HDF5 file containing testing images\n",
    "\n",
    "# Load the training data using the get_dataloader function\n",
    "# This function will return a DataLoader object that handles batching and shuffling\n",
    "train_loader = get_dataloader(\n",
    "    metadata_file=train_metadata_file,  # Path to training metadata file\n",
    "    hdf5_file=train_hdf5_file,  # Path to HDF5 file with training images\n",
    "    batch_size=batch_size,  # Batch size for the DataLoader\n",
    "    mean=mean,  # Mean values for normalization\n",
    "    std=std,  # Standard deviation values for normalization\n",
    "    augment=True  # Apply data augmentation during training\n",
    ")\n",
    "\n",
    "# Load the testing data using the get_dataloader function\n",
    "# This DataLoader will be used for evaluating the model after each epoch\n",
    "test_loader = get_dataloader(\n",
    "    metadata_file=test_metadata_file,  # Path to testing metadata file\n",
    "    hdf5_file=test_hdf5_file,  # Path to HDF5 file with testing images\n",
    "    batch_size=batch_size,  # Batch size for the DataLoader\n",
    "    mean=mean,  # Mean values for normalization\n",
    "    std=std,  # Standard deviation values for normalization\n",
    "    augment=False  # Do not apply data augmentation during testing\n",
    ")\n",
    "\n",
    "# Load the EfficientNet-V2 model with pre-trained weights\n",
    "model = models.efficientnet_v2_m(weights='DEFAULT')\n",
    "\n",
    "# Modify the output layer of the model to match our binary classification task\n",
    "# The original model has an output layer for 1000 classes, so we replace it with a single output unit\n",
    "model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 1)\n",
    "\n",
    "# Train the model using the train_model function\n",
    "# This function will handle the training loop, including forward pass, backward pass, and weight updates\n",
    "train_model(\n",
    "    model=model,  # The model to train\n",
    "    train_loader=train_loader,  # DataLoader for training data\n",
    "    test_loader=test_loader,  # DataLoader for testing data\n",
    "    num_epochs=num_epochs,  # Number of epochs to train\n",
    "    learning_rate=0.001,  # Learning rate for the optimizer\n",
    "    save_path=save_path,  # Path to save the trained model\n",
    "    device=device  # Device to use for training ('cuda' or 'cpu')\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
